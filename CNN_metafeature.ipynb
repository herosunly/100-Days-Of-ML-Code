{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_metafeature.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/herosunly/100-Days-Of-ML-Code/blob/master/CNN_metafeature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Vb_HiFH0w9b7",
        "colab_type": "code",
        "outputId": "0ed432eb-85ed-42da-e1e8-4d2a430180a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l8n_PSGCpLYG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = './drive/My Drive/data/'\n",
        "import pickle\n",
        "train_labels = pickle.load(open(path + 'train_label.pkl', 'rb'))\n",
        "#train_seq = pickle.load(open(path + 'tr_api_type_seq.pkl', 'rb'))\n",
        "#test_seq = pickle.load(open(path + 'te_api_type_seq.pkl', 'rb'))\n",
        "train_seq = pickle.load(open(path + 'tr_seq.pkl', 'rb'))\n",
        "test_seq = pickle.load(open(path + 'te_seq.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "83KJA5itpLap",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Lambda, Embedding, Dropout, Activation, GRU, Bidirectional\n",
        "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling2D, GlobalAveragePooling1D, GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.layers import concatenate, Concatenate, Average, Dot, Maximum, Multiply, Subtract, average\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RG0cD6BypLa7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def TextCNN(max_len,max_cnt,embed_size,\n",
        "            num_filters,kernel_size,\n",
        "            conv_action,\n",
        "            mask_zero):\n",
        "    _input = Input(shape=(max_len,), dtype='int32')\n",
        "    _embed = Embedding(max_cnt, embed_size, input_length=max_len, mask_zero=mask_zero)(_input)\n",
        "    _embed = SpatialDropout1D(0.15)(_embed)\n",
        "    warppers = []\n",
        "    for _kernel_size in kernel_size:\n",
        "        conv1d = Conv1D(filters=num_filters, kernel_size=_kernel_size, activation=conv_action)(_embed)\n",
        "        warppers.append(GlobalMaxPooling1D()(conv1d))\n",
        "                        \n",
        "    fc = concatenate(warppers)\n",
        "    fc = Dropout(0.5)(fc)\n",
        "    #fc = BatchNormalization()(fc)\n",
        "    fc = Dense(256, activation='relu')(fc)\n",
        "    fc = Dropout(0.25)(fc)\n",
        "    #fc = BatchNormalization()(fc) \n",
        "    preds = Dense(8, activation = 'softmax')(fc)\n",
        "    \n",
        "    model = Model(inputs=_input, outputs=preds)\n",
        "    \n",
        "    #model.compile(loss='categorical_crossentropy',\n",
        "    #              optimizer='adam',\n",
        "    #              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4nF_eMLRFbId",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def CNN(max_len, max_cnt, embed_size, mask_zero):\n",
        "    _input = Input(shape=(max_len,), dtype='int32')\n",
        "    _embed = Embedding(max_cnt, embed_size, input_length=max_len, mask_zero=mask_zero)(_input)\n",
        "    _embed = SpatialDropout1D(0.4)(_embed)\n",
        "    \n",
        "    wrappers = []\n",
        "    \n",
        "    for ks in [3, 5, 7, 9, 11]:\n",
        "        conv1d = Conv1D(filters=64, kernel_size=ks, activation='relu')(_embed)\n",
        "        conv1d = Conv1D(filters=64, kernel_size=ks, activation='relu')(conv1d)    \n",
        "        pool1d = MaxPooling1D(2)(conv1d)\n",
        "        conv1d_2 = Conv1D(filters=128, kernel_size=ks, activation='relu')(pool1d)\n",
        "        conv1d_2 = Conv1D(filters=128, kernel_size=ks, activation='relu')(conv1d_2)\n",
        "        pool1d_2 = GlobalAveragePooling1D()(conv1d_2)\n",
        "        wrappers.append(pool1d_2)\n",
        "    \n",
        "    fc = concatenate(wrappers)\n",
        "    fc = Dropout(0.5)(fc)\n",
        "    fc = Dense(256, activation='relu')(fc)\n",
        "    fc = Dropout(0.5)(fc)\n",
        "    #preds = Dense(8, activation='softmax')(fc)\n",
        "    preds = Dense(2, activation='softmax')(fc)\n",
        "        \n",
        "    model = Model(inputs=_input, outputs=preds)\n",
        "    \n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mpFCoPsuOhBx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def CNN_LSTM(max_len, max_cnt, embed_size, mask_zero):\n",
        "    _input = Input(shape=(max_len,), dtype='int32')\n",
        "    _embed = Embedding(max_cnt, embed_size, input_length=max_len, mask_zero=mask_zero)(_input)\n",
        "    _embed = SpatialDropout1D(0.15)(_embed)\n",
        "    \n",
        "    conv1d = Conv1D(filters=60, kernel_size=3, activation='relu')(_embed)\n",
        "    pool1d = MaxPooling1D(2)(conv1d)\n",
        "    conv1d_2 = Conv1D(filters=60, kernel_size=3, activation='relu')(pool1d)\n",
        "    pool1d_2 = MaxPooling1D(2)(conv1d_2)\n",
        "    lstm = LSTM(100, return_sequences=True)(pool1d_2)\n",
        "    pool = GlobalAveragePooling1D()(lstm)\n",
        "    \n",
        "    fc = Dropout(0.5)(pool)\n",
        "    preds = Dense(8, activation='softmax')(fc)\n",
        "        \n",
        "    model = Model(inputs=_input, outputs=preds)\n",
        "    \n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rg8DjERhpLbn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "max_len = 6000\n",
        "max_cnt = 302\n",
        "embed_size = 128\n",
        "#embed_size = 17\n",
        "num_filters = 64\n",
        "kernel_size = [2,4,6,8,10,12,14]\n",
        "conv_action = 'relu'\n",
        "mask_zero = False\n",
        "TRAIN = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "UpUt4r5RpLbr",
        "colab_type": "code",
        "outputId": "d1164457-2c50-4153-caf4-274d3f85f8ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7415
        }
      },
      "cell_type": "code",
      "source": [
        "tf.logging.set_verbosity(0)\n",
        "meta_train = np.zeros(shape = (len(train_seq),2))\n",
        "meta_test = np.zeros(shape = (len(test_seq),2))\n",
        "binary_labels = np.array([[1,0] if i == 0 else [0,1] for i in train_labels.argmax(1)])\n",
        "\n",
        "for i,(tr_ind,te_ind) in enumerate(skf.split(train_seq, binary_labels.argmax(1))):\n",
        "    print('FOLD: ', i + 1)\n",
        "    print(len(te_ind),len(tr_ind))\n",
        "    #model = TextCNN(max_len,max_cnt,embed_size,num_filters,kernel_size,conv_action,mask_zero)\n",
        "    model = CNN(max_len, max_cnt, embed_size, mask_zero)\n",
        "    #model = CNN_LSTM(max_len, max_cnt, 17, mask_zero)\n",
        "    model_name = 'benchmark_cnn_fold_' + str(i)\n",
        "    X_train, X_train_label = train_seq[tr_ind], binary_labels[tr_ind]\n",
        "    X_val, X_val_label = train_seq[te_ind], binary_labels[te_ind]\n",
        "    model_save_path = path + 'model_weight_final/%s_%s.hdf5'%(model_name, embed_size)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "    model_checkpoint = ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "    strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR']))\n",
        "    model = tf.contrib.tpu.keras_to_tpu_model(model, strategy)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    if TRAIN:\n",
        "        model.fit(X_train, X_train_label,\n",
        "                  validation_data=(X_val, X_val_label),\n",
        "                  epochs=100, batch_size=64,\n",
        "                  shuffle=True,\n",
        "                  callbacks=[early_stopping]\n",
        "                 )\n",
        "    \n",
        "    #model.load_weights(model_save_path)\n",
        "    model.save_weights('/tmp/%s.h5' % model_name, overwrite=True)\n",
        "    cpu_model = model.sync_to_cpu()\n",
        "    pred_val = cpu_model.predict(X_val, batch_size=64, verbose=1)\n",
        "    pred_test = cpu_model.predict(test_seq, batch_size=64, verbose=1)\n",
        "    \n",
        "    pred_train = cpu_model.predict(train_seq, batch_size=64, verbose=1)\n",
        "    print(confusion_matrix(binary_labels.argmax(1), pred_train.argmax(1)))\n",
        "        \n",
        "    meta_train[te_ind] = pred_val\n",
        "    meta_test += pred_test\n",
        "    tf.keras.backend.clear_session()\n",
        "meta_test /= 5.0\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD:  1\n",
            "2778 11109\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 11109 samples, validate on 2778 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "11109/11109 [==============================] - 114s 10ms/sample - loss: 0.4857 - acc: 0.7666 - val_loss: 0.4159 - val_acc: 0.8192\n",
            "Epoch 2/100\n",
            "11109/11109 [==============================] - 10s 913us/sample - loss: 0.3597 - acc: 0.8517 - val_loss: 0.3999 - val_acc: 0.8318\n",
            "Epoch 3/100\n",
            "11109/11109 [==============================] - 10s 918us/sample - loss: 0.3259 - acc: 0.8646 - val_loss: 0.3180 - val_acc: 0.8822\n",
            "Epoch 4/100\n",
            "11109/11109 [==============================] - 10s 891us/sample - loss: 0.2877 - acc: 0.8736 - val_loss: 0.2766 - val_acc: 0.8811\n",
            "Epoch 5/100\n",
            "11109/11109 [==============================] - 10s 895us/sample - loss: 0.2384 - acc: 0.8919 - val_loss: 0.2241 - val_acc: 0.9218\n",
            "Epoch 6/100\n",
            "11109/11109 [==============================] - 10s 906us/sample - loss: 0.2031 - acc: 0.9139 - val_loss: 0.2019 - val_acc: 0.9413\n",
            "Epoch 7/100\n",
            "11109/11109 [==============================] - 10s 906us/sample - loss: 0.1801 - acc: 0.9321 - val_loss: 0.2075 - val_acc: 0.9445\n",
            "Epoch 8/100\n",
            "11109/11109 [==============================] - 10s 897us/sample - loss: 0.1680 - acc: 0.9380 - val_loss: 0.1757 - val_acc: 0.9413\n",
            "Epoch 9/100\n",
            "11109/11109 [==============================] - 10s 898us/sample - loss: 0.1534 - acc: 0.9451 - val_loss: 0.1787 - val_acc: 0.9442\n",
            "Epoch 10/100\n",
            "11109/11109 [==============================] - 10s 901us/sample - loss: 0.1385 - acc: 0.9496 - val_loss: 0.1967 - val_acc: 0.9402\n",
            "Epoch 11/100\n",
            "11109/11109 [==============================] - 10s 908us/sample - loss: 0.1245 - acc: 0.9552 - val_loss: 0.1936 - val_acc: 0.9521\n",
            "Epoch 12/100\n",
            "11109/11109 [==============================] - 10s 903us/sample - loss: 0.1159 - acc: 0.9597 - val_loss: 0.1738 - val_acc: 0.9492\n",
            "Epoch 13/100\n",
            "11109/11109 [==============================] - 10s 904us/sample - loss: 0.1164 - acc: 0.9610 - val_loss: 0.1267 - val_acc: 0.9571\n",
            "Epoch 14/100\n",
            "11109/11109 [==============================] - 10s 919us/sample - loss: 0.1004 - acc: 0.9642 - val_loss: 0.1372 - val_acc: 0.9579\n",
            "Epoch 15/100\n",
            "11109/11109 [==============================] - 10s 914us/sample - loss: 0.0982 - acc: 0.9659 - val_loss: 0.1361 - val_acc: 0.9571\n",
            "Epoch 16/100\n",
            "11109/11109 [==============================] - 10s 910us/sample - loss: 0.0877 - acc: 0.9699 - val_loss: 0.1346 - val_acc: 0.9589\n",
            "Epoch 17/100\n",
            "11109/11109 [==============================] - 10s 905us/sample - loss: 0.0910 - acc: 0.9696 - val_loss: 0.1322 - val_acc: 0.9597\n",
            "Epoch 18/100\n",
            "11109/11109 [==============================] - 10s 908us/sample - loss: 0.0851 - acc: 0.9711 - val_loss: 0.1365 - val_acc: 0.9611\n",
            "Epoch 19/100\n",
            "11109/11109 [==============================] - 10s 912us/sample - loss: 0.0925 - acc: 0.9708 - val_loss: 0.1499 - val_acc: 0.9564\n",
            "Epoch 20/100\n",
            "11109/11109 [==============================] - 10s 907us/sample - loss: 0.0836 - acc: 0.9724 - val_loss: 0.1306 - val_acc: 0.9651\n",
            "Epoch 21/100\n",
            "11109/11109 [==============================] - 10s 903us/sample - loss: 0.0725 - acc: 0.9762 - val_loss: 0.1499 - val_acc: 0.9593\n",
            "Epoch 22/100\n",
            "11109/11109 [==============================] - 10s 900us/sample - loss: 0.0682 - acc: 0.9767 - val_loss: 0.1326 - val_acc: 0.9643\n",
            "Epoch 23/100\n",
            "11109/11109 [==============================] - 10s 913us/sample - loss: 0.0644 - acc: 0.9785 - val_loss: 0.1285 - val_acc: 0.9640\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "2778/2778 [==============================] - 23s 8ms/sample\n",
            "12955/12955 [==============================] - 102s 8ms/sample\n",
            "13887/13887 [==============================] - 109s 8ms/sample\n",
            "[[4876  102]\n",
            " [ 163 8746]]\n",
            "FOLD:  2\n",
            "2778 11109\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 11109 samples, validate on 2778 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "11109/11109 [==============================] - 112s 10ms/sample - loss: 0.4732 - acc: 0.7690 - val_loss: 0.4022 - val_acc: 0.8278\n",
            "Epoch 2/100\n",
            "11109/11109 [==============================] - 10s 917us/sample - loss: 0.3533 - acc: 0.8498 - val_loss: 0.3667 - val_acc: 0.8512\n",
            "Epoch 3/100\n",
            "11109/11109 [==============================] - 10s 898us/sample - loss: 0.3238 - acc: 0.8649 - val_loss: 0.3225 - val_acc: 0.8793\n",
            "Epoch 4/100\n",
            "11109/11109 [==============================] - 10s 891us/sample - loss: 0.2882 - acc: 0.8786 - val_loss: 0.2984 - val_acc: 0.8764\n",
            "Epoch 5/100\n",
            "11109/11109 [==============================] - 10s 906us/sample - loss: 0.2359 - acc: 0.8918 - val_loss: 0.6411 - val_acc: 0.7496\n",
            "Epoch 6/100\n",
            "11109/11109 [==============================] - 10s 905us/sample - loss: 0.2136 - acc: 0.9123 - val_loss: 0.3015 - val_acc: 0.9114\n",
            "Epoch 7/100\n",
            "11109/11109 [==============================] - 10s 903us/sample - loss: 0.1808 - acc: 0.9343 - val_loss: 0.2447 - val_acc: 0.9280\n",
            "Epoch 8/100\n",
            "11109/11109 [==============================] - 10s 913us/sample - loss: 0.1582 - acc: 0.9412 - val_loss: 0.2216 - val_acc: 0.9352\n",
            "Epoch 9/100\n",
            "11109/11109 [==============================] - 10s 922us/sample - loss: 0.1495 - acc: 0.9463 - val_loss: 0.1961 - val_acc: 0.9391\n",
            "Epoch 10/100\n",
            "11109/11109 [==============================] - 10s 912us/sample - loss: 0.1317 - acc: 0.9551 - val_loss: 0.1845 - val_acc: 0.9463\n",
            "Epoch 11/100\n",
            "11109/11109 [==============================] - 10s 916us/sample - loss: 0.1203 - acc: 0.9579 - val_loss: 0.1462 - val_acc: 0.9514\n",
            "Epoch 12/100\n",
            "11109/11109 [==============================] - 10s 908us/sample - loss: 0.1078 - acc: 0.9651 - val_loss: 0.1450 - val_acc: 0.9557\n",
            "Epoch 13/100\n",
            "11109/11109 [==============================] - 10s 881us/sample - loss: 0.1066 - acc: 0.9642 - val_loss: 0.1544 - val_acc: 0.9575\n",
            "Epoch 14/100\n",
            "11109/11109 [==============================] - 10s 914us/sample - loss: 0.0959 - acc: 0.9681 - val_loss: 0.1453 - val_acc: 0.9600\n",
            "Epoch 15/100\n",
            "11109/11109 [==============================] - 10s 923us/sample - loss: 0.0859 - acc: 0.9734 - val_loss: 0.1524 - val_acc: 0.9604\n",
            "Epoch 16/100\n",
            "11109/11109 [==============================] - 10s 913us/sample - loss: 0.0846 - acc: 0.9733 - val_loss: 0.1282 - val_acc: 0.9654\n",
            "Epoch 17/100\n",
            "11109/11109 [==============================] - 10s 907us/sample - loss: 0.0769 - acc: 0.9777 - val_loss: 0.1197 - val_acc: 0.9658\n",
            "Epoch 18/100\n",
            "11109/11109 [==============================] - 10s 886us/sample - loss: 0.0751 - acc: 0.9751 - val_loss: 0.1410 - val_acc: 0.9658\n",
            "Epoch 19/100\n",
            "11109/11109 [==============================] - 10s 908us/sample - loss: 0.0786 - acc: 0.9751 - val_loss: 0.1090 - val_acc: 0.9690\n",
            "Epoch 20/100\n",
            "11109/11109 [==============================] - 10s 911us/sample - loss: 0.0607 - acc: 0.9796 - val_loss: 0.1201 - val_acc: 0.9672\n",
            "Epoch 21/100\n",
            "11109/11109 [==============================] - 10s 896us/sample - loss: 0.0606 - acc: 0.9798 - val_loss: 0.1077 - val_acc: 0.9705\n",
            "Epoch 22/100\n",
            "11109/11109 [==============================] - 10s 905us/sample - loss: 0.0575 - acc: 0.9825 - val_loss: 0.1127 - val_acc: 0.9694\n",
            "Epoch 23/100\n",
            "11109/11109 [==============================] - 10s 903us/sample - loss: 0.0578 - acc: 0.9819 - val_loss: 0.1106 - val_acc: 0.9687\n",
            "Epoch 24/100\n",
            "11109/11109 [==============================] - 10s 897us/sample - loss: 0.0513 - acc: 0.9837 - val_loss: 0.1339 - val_acc: 0.9690\n",
            "Epoch 25/100\n",
            "11109/11109 [==============================] - 10s 903us/sample - loss: 0.0583 - acc: 0.9814 - val_loss: 0.1067 - val_acc: 0.9705\n",
            "Epoch 26/100\n",
            "11109/11109 [==============================] - 10s 902us/sample - loss: 0.0458 - acc: 0.9842 - val_loss: 0.1199 - val_acc: 0.9690\n",
            "Epoch 27/100\n",
            "11109/11109 [==============================] - 10s 909us/sample - loss: 0.0467 - acc: 0.9854 - val_loss: 0.1238 - val_acc: 0.9723\n",
            "Epoch 28/100\n",
            "11109/11109 [==============================] - 10s 901us/sample - loss: 0.0439 - acc: 0.9867 - val_loss: 0.1031 - val_acc: 0.9744\n",
            "Epoch 29/100\n",
            "11109/11109 [==============================] - 10s 884us/sample - loss: 0.0384 - acc: 0.9891 - val_loss: 0.1163 - val_acc: 0.9755\n",
            "Epoch 30/100\n",
            "11109/11109 [==============================] - 10s 907us/sample - loss: 0.0379 - acc: 0.9883 - val_loss: 0.1007 - val_acc: 0.9748\n",
            "Epoch 31/100\n",
            "11109/11109 [==============================] - 10s 906us/sample - loss: 0.0367 - acc: 0.9900 - val_loss: 0.1115 - val_acc: 0.9751\n",
            "Epoch 32/100\n",
            "11109/11109 [==============================] - 10s 899us/sample - loss: 0.0383 - acc: 0.9894 - val_loss: 0.1251 - val_acc: 0.9730\n",
            "Epoch 33/100\n",
            "11109/11109 [==============================] - 10s 887us/sample - loss: 0.0419 - acc: 0.9885 - val_loss: 0.1053 - val_acc: 0.9769\n",
            "Epoch 34/100\n",
            "11109/11109 [==============================] - 10s 878us/sample - loss: 0.0326 - acc: 0.9909 - val_loss: 0.1262 - val_acc: 0.9741\n",
            "Epoch 35/100\n",
            "11109/11109 [==============================] - 10s 893us/sample - loss: 0.0324 - acc: 0.9900 - val_loss: 0.1155 - val_acc: 0.9748\n",
            "Epoch 36/100\n",
            "11109/11109 [==============================] - 10s 914us/sample - loss: 0.0279 - acc: 0.9920 - val_loss: 0.1402 - val_acc: 0.9701\n",
            "Epoch 37/100\n",
            "11109/11109 [==============================] - 10s 880us/sample - loss: 0.0281 - acc: 0.9915 - val_loss: 0.1294 - val_acc: 0.9737\n",
            "Epoch 38/100\n",
            "11109/11109 [==============================] - 10s 902us/sample - loss: 0.0298 - acc: 0.9924 - val_loss: 0.1276 - val_acc: 0.9751\n",
            "Epoch 39/100\n",
            "11109/11109 [==============================] - 10s 904us/sample - loss: 0.0279 - acc: 0.9907 - val_loss: 0.1334 - val_acc: 0.9751\n",
            "Epoch 40/100\n",
            "11109/11109 [==============================] - 10s 890us/sample - loss: 0.0347 - acc: 0.9910 - val_loss: 0.1313 - val_acc: 0.9741\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "2778/2778 [==============================] - 29s 10ms/sample\n",
            "12955/12955 [==============================] - 124s 10ms/sample\n",
            "13887/13887 [==============================] - 125s 9ms/sample\n",
            "[[4925   53]\n",
            " [  61 8848]]\n",
            "FOLD:  3\n",
            "2778 11109\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 11109 samples, validate on 2778 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "11109/11109 [==============================] - 111s 10ms/sample - loss: 0.4786 - acc: 0.7730 - val_loss: 0.5409 - val_acc: 0.7637\n",
            "Epoch 2/100\n",
            "11109/11109 [==============================] - 10s 925us/sample - loss: 0.3622 - acc: 0.8497 - val_loss: 0.3595 - val_acc: 0.8505\n",
            "Epoch 3/100\n",
            "11109/11109 [==============================] - 10s 915us/sample - loss: 0.3263 - acc: 0.8647 - val_loss: 0.3144 - val_acc: 0.8606\n",
            "Epoch 4/100\n",
            "11109/11109 [==============================] - 10s 910us/sample - loss: 0.2967 - acc: 0.8731 - val_loss: 0.2959 - val_acc: 0.8728\n",
            "Epoch 5/100\n",
            "11109/11109 [==============================] - 10s 910us/sample - loss: 0.2547 - acc: 0.8893 - val_loss: 0.2784 - val_acc: 0.8757\n",
            "Epoch 6/100\n",
            "11109/11109 [==============================] - 10s 914us/sample - loss: 0.2252 - acc: 0.9013 - val_loss: 0.2741 - val_acc: 0.9078\n",
            "Epoch 7/100\n",
            "11109/11109 [==============================] - 10s 928us/sample - loss: 0.1997 - acc: 0.9106 - val_loss: 0.2687 - val_acc: 0.9067\n",
            "Epoch 8/100\n",
            "11109/11109 [==============================] - 10s 902us/sample - loss: 0.1793 - acc: 0.9262 - val_loss: 0.2015 - val_acc: 0.9344\n",
            "Epoch 9/100\n",
            "11109/11109 [==============================] - 10s 901us/sample - loss: 0.1673 - acc: 0.9372 - val_loss: 0.1969 - val_acc: 0.9348\n",
            "Epoch 10/100\n",
            "11109/11109 [==============================] - 10s 895us/sample - loss: 0.1429 - acc: 0.9465 - val_loss: 0.1988 - val_acc: 0.9395\n",
            "Epoch 11/100\n",
            "11109/11109 [==============================] - 10s 916us/sample - loss: 0.1345 - acc: 0.9517 - val_loss: 0.1760 - val_acc: 0.9445\n",
            "Epoch 12/100\n",
            "11109/11109 [==============================] - 10s 912us/sample - loss: 0.1180 - acc: 0.9566 - val_loss: 0.1485 - val_acc: 0.9488\n",
            "Epoch 13/100\n",
            "11109/11109 [==============================] - 10s 922us/sample - loss: 0.1069 - acc: 0.9625 - val_loss: 0.1708 - val_acc: 0.9510\n",
            "Epoch 14/100\n",
            "11109/11109 [==============================] - 10s 905us/sample - loss: 0.1034 - acc: 0.9645 - val_loss: 0.1652 - val_acc: 0.9535\n",
            "Epoch 15/100\n",
            "11109/11109 [==============================] - 10s 909us/sample - loss: 0.1084 - acc: 0.9630 - val_loss: 0.1491 - val_acc: 0.9564\n",
            "Epoch 16/100\n",
            "11109/11109 [==============================] - 10s 892us/sample - loss: 0.0902 - acc: 0.9673 - val_loss: 0.1531 - val_acc: 0.9582\n",
            "Epoch 17/100\n",
            "11109/11109 [==============================] - 10s 908us/sample - loss: 0.0911 - acc: 0.9704 - val_loss: 0.1700 - val_acc: 0.9543\n",
            "Epoch 18/100\n",
            "11109/11109 [==============================] - 10s 899us/sample - loss: 0.0835 - acc: 0.9734 - val_loss: 0.1715 - val_acc: 0.9503\n",
            "Epoch 19/100\n",
            "11109/11109 [==============================] - 10s 911us/sample - loss: 0.0846 - acc: 0.9711 - val_loss: 0.1585 - val_acc: 0.9579\n",
            "Epoch 20/100\n",
            "11109/11109 [==============================] - 10s 902us/sample - loss: 0.0824 - acc: 0.9733 - val_loss: 0.1293 - val_acc: 0.9575\n",
            "Epoch 21/100\n",
            "11109/11109 [==============================] - 10s 905us/sample - loss: 0.0713 - acc: 0.9743 - val_loss: 0.1417 - val_acc: 0.9600\n",
            "Epoch 22/100\n",
            "11109/11109 [==============================] - 10s 915us/sample - loss: 0.0710 - acc: 0.9759 - val_loss: 0.1340 - val_acc: 0.9589\n",
            "Epoch 23/100\n",
            "11109/11109 [==============================] - 10s 896us/sample - loss: 0.0716 - acc: 0.9773 - val_loss: 0.1479 - val_acc: 0.9593\n",
            "Epoch 24/100\n",
            "11109/11109 [==============================] - 10s 882us/sample - loss: 0.0599 - acc: 0.9819 - val_loss: 0.1534 - val_acc: 0.9600\n",
            "Epoch 25/100\n",
            "11109/11109 [==============================] - 10s 911us/sample - loss: 0.0551 - acc: 0.9804 - val_loss: 0.1661 - val_acc: 0.9582\n",
            "Epoch 26/100\n",
            "11109/11109 [==============================] - 10s 917us/sample - loss: 0.0582 - acc: 0.9797 - val_loss: 0.1465 - val_acc: 0.9618\n",
            "Epoch 27/100\n",
            "11109/11109 [==============================] - 10s 898us/sample - loss: 0.0491 - acc: 0.9842 - val_loss: 0.1655 - val_acc: 0.9582\n",
            "Epoch 28/100\n",
            "11109/11109 [==============================] - 10s 901us/sample - loss: 0.0588 - acc: 0.9805 - val_loss: 0.1642 - val_acc: 0.9640\n",
            "Epoch 29/100\n",
            "11109/11109 [==============================] - 10s 901us/sample - loss: 0.0488 - acc: 0.9828 - val_loss: 0.1721 - val_acc: 0.9647\n",
            "Epoch 30/100\n",
            "11109/11109 [==============================] - 10s 906us/sample - loss: 0.0466 - acc: 0.9844 - val_loss: 0.1626 - val_acc: 0.9640\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "2778/2778 [==============================] - 23s 8ms/sample\n",
            "12955/12955 [==============================] - 102s 8ms/sample\n",
            "13887/13887 [==============================] - 109s 8ms/sample\n",
            "[[4894   84]\n",
            " [ 128 8781]]\n",
            "FOLD:  4\n",
            "2777 11110\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 11110 samples, validate on 2777 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "11110/11110 [==============================] - 114s 10ms/sample - loss: 0.4709 - acc: 0.7733 - val_loss: 0.3930 - val_acc: 0.8480\n",
            "Epoch 2/100\n",
            "11110/11110 [==============================] - 10s 924us/sample - loss: 0.3526 - acc: 0.8528 - val_loss: 0.3592 - val_acc: 0.8613\n",
            "Epoch 3/100\n",
            "11110/11110 [==============================] - 10s 925us/sample - loss: 0.3171 - acc: 0.8676 - val_loss: 0.5727 - val_acc: 0.8040\n",
            "Epoch 4/100\n",
            "11110/11110 [==============================] - 11s 948us/sample - loss: 0.2854 - acc: 0.8781 - val_loss: 0.5221 - val_acc: 0.8084\n",
            "Epoch 5/100\n",
            "11110/11110 [==============================] - 10s 914us/sample - loss: 0.2468 - acc: 0.9017 - val_loss: 0.3832 - val_acc: 0.8793\n",
            "Epoch 6/100\n",
            "11110/11110 [==============================] - 10s 938us/sample - loss: 0.2136 - acc: 0.9198 - val_loss: 0.2892 - val_acc: 0.9107\n",
            "Epoch 7/100\n",
            "11110/11110 [==============================] - 10s 914us/sample - loss: 0.1811 - acc: 0.9351 - val_loss: 0.2526 - val_acc: 0.9146\n",
            "Epoch 8/100\n",
            "11110/11110 [==============================] - 10s 929us/sample - loss: 0.1647 - acc: 0.9424 - val_loss: 0.2152 - val_acc: 0.9373\n",
            "Epoch 9/100\n",
            "11110/11110 [==============================] - 10s 913us/sample - loss: 0.1496 - acc: 0.9490 - val_loss: 0.2190 - val_acc: 0.9308\n",
            "Epoch 10/100\n",
            "11110/11110 [==============================] - 10s 918us/sample - loss: 0.1391 - acc: 0.9529 - val_loss: 0.2705 - val_acc: 0.9218\n",
            "Epoch 11/100\n",
            "11110/11110 [==============================] - 10s 904us/sample - loss: 0.1273 - acc: 0.9599 - val_loss: 0.2110 - val_acc: 0.9377\n",
            "Epoch 12/100\n",
            "11110/11110 [==============================] - 10s 931us/sample - loss: 0.1207 - acc: 0.9623 - val_loss: 0.2686 - val_acc: 0.9269\n",
            "Epoch 13/100\n",
            "11110/11110 [==============================] - 10s 902us/sample - loss: 0.1166 - acc: 0.9632 - val_loss: 0.2303 - val_acc: 0.9380\n",
            "Epoch 14/100\n",
            "11110/11110 [==============================] - 10s 930us/sample - loss: 0.1079 - acc: 0.9650 - val_loss: 0.1904 - val_acc: 0.9460\n",
            "Epoch 15/100\n",
            "11110/11110 [==============================] - 10s 923us/sample - loss: 0.1040 - acc: 0.9674 - val_loss: 0.2378 - val_acc: 0.9362\n",
            "Epoch 16/100\n",
            "11110/11110 [==============================] - 10s 908us/sample - loss: 0.1017 - acc: 0.9671 - val_loss: 0.1907 - val_acc: 0.9481\n",
            "Epoch 17/100\n",
            "11110/11110 [==============================] - 10s 907us/sample - loss: 0.0879 - acc: 0.9731 - val_loss: 0.1751 - val_acc: 0.9561\n",
            "Epoch 18/100\n",
            "11110/11110 [==============================] - 10s 919us/sample - loss: 0.0864 - acc: 0.9738 - val_loss: 0.1738 - val_acc: 0.9460\n",
            "Epoch 19/100\n",
            "11110/11110 [==============================] - 10s 921us/sample - loss: 0.0811 - acc: 0.9757 - val_loss: 0.1592 - val_acc: 0.9579\n",
            "Epoch 20/100\n",
            "11110/11110 [==============================] - 11s 946us/sample - loss: 0.0808 - acc: 0.9760 - val_loss: 0.1645 - val_acc: 0.9550\n",
            "Epoch 21/100\n",
            "11110/11110 [==============================] - 10s 938us/sample - loss: 0.0850 - acc: 0.9743 - val_loss: 0.1632 - val_acc: 0.9561\n",
            "Epoch 22/100\n",
            "11110/11110 [==============================] - 10s 930us/sample - loss: 0.0685 - acc: 0.9814 - val_loss: 0.1858 - val_acc: 0.9553\n",
            "Epoch 23/100\n",
            "11110/11110 [==============================] - 10s 919us/sample - loss: 0.0723 - acc: 0.9791 - val_loss: 0.1872 - val_acc: 0.9586\n",
            "Epoch 24/100\n",
            "11110/11110 [==============================] - 10s 899us/sample - loss: 0.0745 - acc: 0.9777 - val_loss: 0.1749 - val_acc: 0.9593\n",
            "Epoch 25/100\n",
            "11110/11110 [==============================] - 10s 912us/sample - loss: 0.0821 - acc: 0.9791 - val_loss: 0.2158 - val_acc: 0.9597\n",
            "Epoch 26/100\n",
            "11110/11110 [==============================] - 10s 907us/sample - loss: 0.0718 - acc: 0.9790 - val_loss: 0.1725 - val_acc: 0.9589\n",
            "Epoch 27/100\n",
            "11110/11110 [==============================] - 10s 917us/sample - loss: 0.0630 - acc: 0.9801 - val_loss: 0.1611 - val_acc: 0.9593\n",
            "Epoch 28/100\n",
            "11110/11110 [==============================] - 10s 924us/sample - loss: 0.0556 - acc: 0.9846 - val_loss: 0.1930 - val_acc: 0.9571\n",
            "Epoch 29/100\n",
            "11110/11110 [==============================] - 10s 928us/sample - loss: 0.0628 - acc: 0.9838 - val_loss: 0.1549 - val_acc: 0.9636\n",
            "Epoch 30/100\n",
            "11110/11110 [==============================] - 10s 912us/sample - loss: 0.0527 - acc: 0.9867 - val_loss: 0.1761 - val_acc: 0.9618\n",
            "Epoch 31/100\n",
            "11110/11110 [==============================] - 10s 936us/sample - loss: 0.0554 - acc: 0.9832 - val_loss: 0.1642 - val_acc: 0.9575\n",
            "Epoch 32/100\n",
            "11110/11110 [==============================] - 10s 931us/sample - loss: 0.0522 - acc: 0.9851 - val_loss: 0.2021 - val_acc: 0.9514\n",
            "Epoch 33/100\n",
            "11110/11110 [==============================] - 10s 934us/sample - loss: 0.0541 - acc: 0.9846 - val_loss: 0.1807 - val_acc: 0.9618\n",
            "Epoch 34/100\n",
            "11110/11110 [==============================] - 11s 955us/sample - loss: 0.0476 - acc: 0.9871 - val_loss: 0.1708 - val_acc: 0.9597\n",
            "Epoch 35/100\n",
            "11110/11110 [==============================] - 10s 915us/sample - loss: 0.0457 - acc: 0.9873 - val_loss: 0.1687 - val_acc: 0.9643\n",
            "Epoch 36/100\n",
            "11110/11110 [==============================] - 10s 929us/sample - loss: 0.0531 - acc: 0.9869 - val_loss: 0.1683 - val_acc: 0.9625\n",
            "Epoch 37/100\n",
            "11110/11110 [==============================] - 10s 910us/sample - loss: 0.0530 - acc: 0.9841 - val_loss: 0.1501 - val_acc: 0.9597\n",
            "Epoch 38/100\n",
            "11110/11110 [==============================] - 10s 916us/sample - loss: 0.0430 - acc: 0.9880 - val_loss: 0.1448 - val_acc: 0.9604\n",
            "Epoch 39/100\n",
            "11110/11110 [==============================] - 10s 910us/sample - loss: 0.0374 - acc: 0.9894 - val_loss: 0.1272 - val_acc: 0.9622\n",
            "Epoch 40/100\n",
            "11110/11110 [==============================] - 10s 916us/sample - loss: 0.0438 - acc: 0.9869 - val_loss: 0.1505 - val_acc: 0.9575\n",
            "Epoch 41/100\n",
            "11110/11110 [==============================] - 10s 933us/sample - loss: 0.0404 - acc: 0.9887 - val_loss: 0.1594 - val_acc: 0.9589\n",
            "Epoch 42/100\n",
            "11110/11110 [==============================] - 10s 945us/sample - loss: 0.0417 - acc: 0.9880 - val_loss: 0.2008 - val_acc: 0.9517\n",
            "Epoch 43/100\n",
            "11110/11110 [==============================] - 10s 938us/sample - loss: 0.0432 - acc: 0.9870 - val_loss: 0.1612 - val_acc: 0.9593\n",
            "Epoch 44/100\n",
            "11110/11110 [==============================] - 10s 930us/sample - loss: 0.0346 - acc: 0.9904 - val_loss: 0.1817 - val_acc: 0.9633\n",
            "Epoch 45/100\n",
            "11110/11110 [==============================] - 10s 914us/sample - loss: 0.0434 - acc: 0.9886 - val_loss: 0.1751 - val_acc: 0.9640\n",
            "Epoch 46/100\n",
            "11110/11110 [==============================] - 10s 922us/sample - loss: 0.0328 - acc: 0.9907 - val_loss: 0.1920 - val_acc: 0.9593\n",
            "Epoch 47/100\n",
            "11110/11110 [==============================] - 10s 909us/sample - loss: 0.0308 - acc: 0.9905 - val_loss: 0.2098 - val_acc: 0.9622\n",
            "Epoch 48/100\n",
            "11110/11110 [==============================] - 10s 910us/sample - loss: 0.0326 - acc: 0.9901 - val_loss: 0.1903 - val_acc: 0.9643\n",
            "Epoch 49/100\n",
            "11110/11110 [==============================] - 10s 912us/sample - loss: 0.0332 - acc: 0.9897 - val_loss: 0.1942 - val_acc: 0.9586\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "2777/2777 [==============================] - 24s 9ms/sample\n",
            "12955/12955 [==============================] - 105s 8ms/sample\n",
            "13887/13887 [==============================] - 113s 8ms/sample\n",
            "[[4864  114]\n",
            " [  50 8859]]\n",
            "FOLD:  5\n",
            "2776 11111\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Train on 11111 samples, validate on 2776 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "11111/11111 [==============================] - 120s 11ms/sample - loss: 0.5030 - acc: 0.7505 - val_loss: 0.3664 - val_acc: 0.8347\n",
            "Epoch 2/100\n",
            "11111/11111 [==============================] - 10s 934us/sample - loss: 0.3654 - acc: 0.8517 - val_loss: 0.3390 - val_acc: 0.8591\n",
            "Epoch 3/100\n",
            "11111/11111 [==============================] - 10s 920us/sample - loss: 0.3244 - acc: 0.8629 - val_loss: 0.3432 - val_acc: 0.8595\n",
            "Epoch 4/100\n",
            "11111/11111 [==============================] - 10s 910us/sample - loss: 0.2977 - acc: 0.8741 - val_loss: 0.2812 - val_acc: 0.8754\n",
            "Epoch 5/100\n",
            "11111/11111 [==============================] - 10s 924us/sample - loss: 0.2511 - acc: 0.8825 - val_loss: 0.2558 - val_acc: 0.8905\n",
            "Epoch 6/100\n",
            "11111/11111 [==============================] - 10s 924us/sample - loss: 0.2213 - acc: 0.9035 - val_loss: 0.2437 - val_acc: 0.8901\n",
            "Epoch 7/100\n",
            "11111/11111 [==============================] - 10s 920us/sample - loss: 0.1938 - acc: 0.9202 - val_loss: 0.2238 - val_acc: 0.9352\n",
            "Epoch 8/100\n",
            "11111/11111 [==============================] - 10s 926us/sample - loss: 0.1748 - acc: 0.9352 - val_loss: 0.1657 - val_acc: 0.9409\n",
            "Epoch 9/100\n",
            "11111/11111 [==============================] - 10s 918us/sample - loss: 0.1683 - acc: 0.9394 - val_loss: 0.1591 - val_acc: 0.9409\n",
            "Epoch 10/100\n",
            "11111/11111 [==============================] - 10s 916us/sample - loss: 0.1493 - acc: 0.9438 - val_loss: 0.1720 - val_acc: 0.9398\n",
            "Epoch 11/100\n",
            "11111/11111 [==============================] - 10s 920us/sample - loss: 0.1332 - acc: 0.9506 - val_loss: 0.1736 - val_acc: 0.9427\n",
            "Epoch 12/100\n",
            "11111/11111 [==============================] - 10s 908us/sample - loss: 0.1194 - acc: 0.9593 - val_loss: 0.1477 - val_acc: 0.9499\n",
            "Epoch 13/100\n",
            "11111/11111 [==============================] - 10s 926us/sample - loss: 0.1236 - acc: 0.9579 - val_loss: 0.1475 - val_acc: 0.9550\n",
            "Epoch 14/100\n",
            "11111/11111 [==============================] - 10s 932us/sample - loss: 0.1118 - acc: 0.9628 - val_loss: 0.1255 - val_acc: 0.9575\n",
            "Epoch 15/100\n",
            "11111/11111 [==============================] - 10s 938us/sample - loss: 0.1032 - acc: 0.9632 - val_loss: 0.1760 - val_acc: 0.9514\n",
            "Epoch 16/100\n",
            "11111/11111 [==============================] - 10s 941us/sample - loss: 0.1007 - acc: 0.9662 - val_loss: 0.1157 - val_acc: 0.9597\n",
            "Epoch 17/100\n",
            "11111/11111 [==============================] - 10s 934us/sample - loss: 0.0937 - acc: 0.9688 - val_loss: 0.1349 - val_acc: 0.9597\n",
            "Epoch 18/100\n",
            "11111/11111 [==============================] - 10s 940us/sample - loss: 0.0898 - acc: 0.9697 - val_loss: 0.1423 - val_acc: 0.9553\n",
            "Epoch 19/100\n",
            "11111/11111 [==============================] - 11s 946us/sample - loss: 0.0906 - acc: 0.9668 - val_loss: 0.1234 - val_acc: 0.9625\n",
            "Epoch 20/100\n",
            "11111/11111 [==============================] - 10s 929us/sample - loss: 0.0803 - acc: 0.9751 - val_loss: 0.1340 - val_acc: 0.9615\n",
            "Epoch 21/100\n",
            "11111/11111 [==============================] - 10s 909us/sample - loss: 0.0742 - acc: 0.9751 - val_loss: 0.1293 - val_acc: 0.9622\n",
            "Epoch 22/100\n",
            "11111/11111 [==============================] - 10s 922us/sample - loss: 0.0657 - acc: 0.9775 - val_loss: 0.1456 - val_acc: 0.9651\n",
            "Epoch 23/100\n",
            "11111/11111 [==============================] - 10s 936us/sample - loss: 0.0730 - acc: 0.9760 - val_loss: 0.1204 - val_acc: 0.9676\n",
            "Epoch 24/100\n",
            "11111/11111 [==============================] - 10s 909us/sample - loss: 0.0664 - acc: 0.9769 - val_loss: 0.1370 - val_acc: 0.9683\n",
            "Epoch 25/100\n",
            "11111/11111 [==============================] - 10s 932us/sample - loss: 0.0601 - acc: 0.9807 - val_loss: 0.1585 - val_acc: 0.9633\n",
            "Epoch 26/100\n",
            "11111/11111 [==============================] - 10s 932us/sample - loss: 0.0656 - acc: 0.9776 - val_loss: 0.1504 - val_acc: 0.9622\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "2776/2776 [==============================] - 23s 8ms/sample\n",
            "12955/12955 [==============================] - 101s 8ms/sample\n",
            "13887/13887 [==============================] - 109s 8ms/sample\n",
            "[[4922   56]\n",
            " [ 196 8713]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EuRCO0KxpLbw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.to_pickle(meta_train, path + 'train_meta_cnn.pkl')\n",
        "pd.to_pickle(meta_test, path + 'test_meta_cnn.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}